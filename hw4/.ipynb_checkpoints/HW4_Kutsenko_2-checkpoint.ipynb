{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:18pt;padding-top:20px; text-align:center\">Домашнее задание 4. <b>Рекомендательные системы и Spark MLlib</b> </div><hr>\n",
    "<div style=\"text-align:right;\">Куценко А. А <span style=\"font-style: italic;font-weight: bold;\">(ftruf357ft@gmail.com)</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Вариант\n",
    "eu_position = 8\n",
    "eu_position % 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2. Коллаборативная фильтрация\n",
    "\n",
    "- Вариант 1. По схожести пользователей\n",
    "- Вариант 2. По схожести объектов\n",
    "\n",
    "Этапы:\n",
    "1. Разделите данные с рейтингами на обучающее (train_init - 0.8) и тестовое подмножества (test - 0.2), определите среднее значение рейтинга в обучающем подмножестве и вычислите `rmse` для тестового подмножества, если для всех значений из test предсказывается среднее значение рейтинга\n",
    "2. Реализуйте коллаборативную фильтрацию в соответствии с вариантом. Для определения схожести используйте train_init, для расчета `rmse` - test\n",
    "3. Определите `rmse` для тестового подмножества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"]=\"/home/ubuntu/BigData/spark\"\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/home/ubuntu/ML/anaconda3/bin/python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"/home/ubuntu/ML/anaconda3/bin/python\"\n",
    "\n",
    "spark_home = os.environ.get(\"SPARK_HOME\")\n",
    "sys.path.insert(0, os.path.join(spark_home, \"python\"))\n",
    "sys.path.insert(0, os.path.join(spark_home, \"python/lib/py4j-0.10.7-src.zip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import Row, RDD\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemBasedRecommend:\n",
    "    \"\"\"\n",
    "    Main class that implements the item-based collaborative filtering.\n",
    "\n",
    "    The approach includes:\n",
    "    - cosine similarity calculation between items\n",
    "    - weighted sum calculation for rating prediction\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    top_N_similarities : int, optional (default=20)\n",
    "        Number of top similarities for a given item pair that will compose\n",
    "        a similarity matrix. It is used in the train phase\n",
    "    top_N_ratings : int, optional (default=10)\n",
    "        Number of top ratings that return as a result of prediction for a\n",
    "        given user\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    spark : SparkSession\n",
    "    top_N_similarities : int\n",
    "    top_N_ratings : int\n",
    "    df_train : DataFrame\n",
    "        Ratings of users in the following format: [usedId, movieId, rating]\n",
    "    br_similarity : Broadcast\n",
    "        Dictionary of similarities of item pairs\n",
    "    br_P : Broadcast\n",
    "        Set of items that contains df_train\n",
    "    br_U : Broadcast\n",
    "        Set of users  that contains df_train\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, spark, top_N_similarities=20, top_N_ratings=None):\n",
    "        self.spark = spark\n",
    "        self.top_N_similarities = int(top_N_similarities)\n",
    "        self.top_N_ratings = int(top_N_ratings) if top_N_ratings else None\n",
    "        \n",
    "    def train(self, df_train, top_N=None, user_column=\"user\", item_column=\"item\", rating_column=\"rating\"):\n",
    "        top_N = int(top_N or self.top_N_similarities)\n",
    "\n",
    "        df = df_train.select(\n",
    "            F.col(user_column).alias(\"user\"),\n",
    "            F.col(item_column).alias(\"item\"),\n",
    "            F.col(rating_column).alias(\"rating\")\n",
    "        ).repartition(\"user\").cache()\n",
    "\n",
    "        # Нормы\n",
    "        df_norm = df.groupBy(\"item\").agg(F.sqrt(F.sum(F.col(\"rating\")**2)).alias(\"norm\"))\n",
    "\n",
    "        # Перемножение рейтингов\n",
    "        df_dot = (\n",
    "            df.alias(\"a\")\n",
    "            .join(df.alias(\"b\"), \"user\")\n",
    "            .where(F.col(\"a.item\") < F.col(\"b.item\"))\n",
    "            .groupBy(F.col(\"a.item\").alias(\"p1\"), F.col(\"b.item\").alias(\"p2\"))\n",
    "            .agg(F.sum(F.col(\"a.rating\") * F.col(\"b.rating\")).alias(\"dot\"))\n",
    "        )\n",
    "\n",
    "        # Broadcast join с нормами\n",
    "        df_norm_cached = df_norm.persist()\n",
    "\n",
    "        df_sim = (\n",
    "            df_dot\n",
    "            .join(F.broadcast(df_norm_cached.withColumnRenamed(\"item\", \"p1\").withColumnRenamed(\"norm\", \"n1\")), \"p1\")\n",
    "            .join(F.broadcast(df_norm_cached.withColumnRenamed(\"item\", \"p2\").withColumnRenamed(\"norm\", \"n2\")), \"p2\")\n",
    "            .select(\n",
    "                \"p1\", \"p2\",\n",
    "                (F.col(\"dot\") / (F.col(\"n1\") * F.col(\"n2\"))).alias(\"sim\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # top-N\n",
    "        window = Window.partitionBy(\"p1\").orderBy(F.col(\"sim\").desc())\n",
    "        df_topN = df_sim.withColumn(\"rn\", F.row_number().over(window)).filter(F.col(\"rn\") <= top_N)\n",
    "\n",
    "        # Сохраняем результат\n",
    "        df_topN.write.mode(\"overwrite\").parquet(\"hdfs:///models/similarity_matrix\")\n",
    "        \n",
    "        P = set(row[\"item\"] for row in df.select(\"item\").distinct().collect())\n",
    "        U = set(row[\"user\"] for row in df.select(\"user\").distinct().collect())\n",
    "\n",
    "        self.br_P = self.spark.sparkContext.broadcast(P)\n",
    "        self.br_U = self.spark.sparkContext.broadcast(U)\n",
    "        self.df_train = df\n",
    "        self.top_N_similarities = top_N\n",
    "        return self\n",
    "        \n",
    "#     def train(self, df_train, top_N=None, user_column_name=\"user\", item_column_name=\"item\",\n",
    "#               rating_column_name=\"rating\"):\n",
    "#         \"\"\"\n",
    "#         Calculate cosine similarities between all item pairs\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         df_train : DataFrame\n",
    "#             Ratings of users in the following format: [usedId, movieId, rating]\n",
    "#         top_N : int or None\n",
    "#             Number of top similarities for a given item pair that will compose\n",
    "#             a similarity matrix. It is used in the train phase\n",
    "#         rating_column_name : str\n",
    "#         user_column_name : str\n",
    "#         item_column_name : str\n",
    "\n",
    "#         Returns\n",
    "#         -------\n",
    "#         self\n",
    "#         \"\"\"\n",
    "#         top_N = int(top_N) if top_N else self.top_N_similarities\n",
    "#         user_column_name = str(user_column_name)\n",
    "#         item_column_name = str(item_column_name)\n",
    "#         rating_column_name = str(rating_column_name)\n",
    "\n",
    "#         clmn_names = [F.col(user_column_name).alias(\"user\"),\n",
    "#                       F.col(item_column_name).alias(\"item\"),\n",
    "#                       F.col(rating_column_name).alias(\"rating\")]\n",
    "\n",
    "#         df_train = df_train.select(clmn_names)\n",
    "\n",
    "#         left_clmn_names = [F.col(\"user\").alias(\"u\"),\n",
    "#                    F.col(\"item\").alias(\"p1\"),\n",
    "#                    F.col(\"rating\").alias(\"v1\")]\n",
    "\n",
    "#         right_clmn_names = [F.col(\"user\").alias(\"u\"),\n",
    "#                     F.col(\"item\").alias(\"p2\"),\n",
    "#                     F.col(\"rating\").alias(\"v2\")]\n",
    "        \n",
    "#         # Step 1. Create dot products\n",
    "        \n",
    "#         df_dot = df_train.select(left_clmn_names)\\\n",
    "#             .join(df_train.select(right_clmn_names), on=\"u\")\\\n",
    "#             .where(F.col(\"p1\") < F.col(\"p2\"))\\\n",
    "#             .groupBy([F.col(\"p1\"), F.col(\"p2\")])\\\n",
    "#             .agg(F.sum(F.col(\"v1\") * F.col(\"v2\")).alias(\"dot\"))\n",
    "\n",
    "#         # Step 2. Calculate norms\n",
    "        \n",
    "#         df_norm = df_train.select(left_clmn_names)\\\n",
    "#             .groupBy(F.col(\"p1\"))\\\n",
    "#             .agg(F.sqrt(F.sum(F.col(\"v1\") * F.col(\"v1\"))).alias(\"norm\"))\n",
    "\n",
    "#         similarity_clmns = [F.col(\"p1\"), F.col(\"p2\"), (F.col(\"dot\")/F.col(\"n1\")/F.col(\"n2\")).alias(\"sim\")]\n",
    "        \n",
    "#         # Step 3. Calculate similarities\n",
    "        \n",
    "#         df_similarity = df_dot.join(df_norm.select(F.col(\"p1\"), F.col(\"norm\").alias(\"n1\")), on=\"p1\")\\\n",
    "#                     .join(df_norm.select(F.col(\"p1\").alias(\"p2\"), F.col(\"norm\").alias(\"n2\")), on=\"p2\")\\\n",
    "#                     .select(similarity_clmns)\n",
    "        \n",
    "#         # Step 4. Truncate similarities\n",
    "\n",
    "#         window = Window.partitionBy(df_similarity[\"p1\"]).orderBy(df_similarity[\"sim\"].desc())\n",
    "#         df_similarity_N = df_similarity.select(\"*\", F.rank().over(window).alias(\"rank\"))\\\n",
    "#                     .filter(F.col(\"rank\") <= top_N)\n",
    "        \n",
    "#         # Step 5. Collect data (similarities, users, products) on driver\n",
    "        \n",
    "#         df_similarity_pn = df_similarity_N.toPandas()\n",
    "#         dict_similarity = df_similarity_pn.set_index([\"p1\", \"p2\"]).to_dict()[\"sim\"]\n",
    "        \n",
    "#         P = {el[\"item\"] for el in df_train[[F.col(\"item\")]].distinct().collect()}\n",
    "#         U = {el[\"user\"] for el in df_train[[F.col(\"user\")]].distinct().collect()}\n",
    "        \n",
    "#         # Step 6. Broadcast data\n",
    "\n",
    "#         self.br_similarity = self.spark.sparkContext.broadcast(dict_similarity)\n",
    "#         self.br_P = self.spark.sparkContext.broadcast(P)\n",
    "#         self.br_U = self.spark.sparkContext.broadcast(U)\n",
    "        \n",
    "#         self.top_N_similarities = top_N\n",
    "\n",
    "#         self.df_train = df_train.persist()\n",
    "\n",
    "#         return self\n",
    "        \n",
    "    def predict(self, user_id, item_id):\n",
    "        \"\"\"Predict rating for a given user and item\"\"\"\n",
    "        raise Exception(\"Not implemented.\")\n",
    "    \n",
    "    def recommend(self, user_ids, top_N_ratings=None, partition_num=20, grouped=True):\n",
    "        \"\"\"\n",
    "        Recommend top N items to given users.\n",
    "\n",
    "        Note: Returns non-zero evaluated (predicted) ratings\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        user_ids : list or set\n",
    "            List of users for which we want to get recommendations\n",
    "        top_N_ratings : int\n",
    "            Number of top ratings that return as a result of prediction for a\n",
    "            given user\n",
    "        partition_num : int\n",
    "            Number of partitions (tasks) to use to calculate predictions\n",
    "        grouped : boolean\n",
    "            If True then the method returns value will be a RDD with the following\n",
    "            format:\n",
    "                [(used_id : [(movieId, evaluated rating),.. ]),.. ]\n",
    "            Otherwise a result will be DataFrame with the following format:\n",
    "                [userId, movieId, evaluated ratings]\n",
    "        Returns\n",
    "        -------\n",
    "        RDD or DataFrame\n",
    "        \"\"\"\n",
    "\n",
    "        top_N_ratings = int(top_N_ratings) if top_N_ratings else self.top_N_ratings\n",
    "\n",
    "        if not hasattr(self, \"df_train\"):\n",
    "            raise Exception(\"It seems you haven't trained the model.\")\n",
    "\n",
    "        # Remove reference to the current class instance\n",
    "        _predict_per_partition = ItemBasedRecommend._predict_per_partition\n",
    "        br_similarity = self.br_similarity\n",
    "        br_P = self.br_P\n",
    "\n",
    "        # Predict ratings through the chain of transformations\n",
    "        rdd_ratings_pred = self.df_train.where(F.col(\"user\").isin(user_ids))\\\n",
    "                .repartition(partition_num, F.col(\"user\"))\\\n",
    "                .sortWithinPartitions(F.col(\"user\")).rdd\\\n",
    "                .mapPartitions(_predict_per_partition(br_similarity, br_P, top_N_ratings, grouped))\n",
    "        \n",
    "        return rdd_ratings_pred if grouped else rdd_ratings_pred.toDF()\n",
    "\n",
    "    @staticmethod\n",
    "    def _predict_per_partition(br_similarity, br_P, top_N_ratings, grouped):\n",
    "        \"\"\"Wrapper for prediction of ratings per partition\"\"\"\n",
    "\n",
    "        def _predict(p_i, PR_u):\n",
    "            \"\"\"Predict a rating for a given user and item\"\"\"\n",
    "            wsum_num = 0.0\n",
    "            wsum_den = 0.0\n",
    "\n",
    "            for p_j_r, r_j_r in PR_u.items():\n",
    "                if p_j_r > p_i and (p_i, p_j_r) in br_similarity.value:\n",
    "                    sim = br_similarity.value[(p_i, p_j_r)]\n",
    "                    wsum_num += sim * r_j_r\n",
    "                    wsum_den += sim\n",
    "                elif p_j_r < p_i and (p_j_r, p_i) in br_similarity.value:\n",
    "                    sim = br_similarity.value[(p_j_r, p_i)]\n",
    "                    wsum_num += sim * r_j_r\n",
    "                    wsum_den += sim\n",
    "\n",
    "            return wsum_num/wsum_den if wsum_den > 0 else 0.0\n",
    "        \n",
    "        def _predict_per_user(PR_u):\n",
    "            \"\"\"\n",
    "            Predict ratings for a given user\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            PR_u : list or set\n",
    "                List of tuple (item, rating), where items are those to which the user has set ratings\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "                List of evaluated ratings\n",
    "\n",
    "            \"\"\"\n",
    "            result = list()\n",
    "            for p_i in br_P.value:\n",
    "                if p_i not in PR_u:\n",
    "                    pred = _predict(p_i, PR_u)\n",
    "                    if pred > 0:\n",
    "                        result.append((p_i, pred))\n",
    "            return result\n",
    "        \n",
    "        def _predict_per_partition_inner_grouped(ratings):\n",
    "            \"\"\"\n",
    "            Predict per partition if grouped option is enabled\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            RDD\n",
    "            \"\"\"\n",
    "            prev_user = None\n",
    "            curr_user = None\n",
    "            for r in ratings:\n",
    "                curr_user = r[\"user\"]\n",
    "                if prev_user == curr_user:\n",
    "                    PR_u[r[\"item\"]] = r[\"rating\"]\n",
    "                else:\n",
    "                    if prev_user:\n",
    "                        yield prev_user, sorted(_predict_per_user(PR_u), key=lambda x: -x[1])[:top_N_ratings]\n",
    "                    PR_u = dict()\n",
    "                    PR_u[r[\"item\"]] = r[\"rating\"]\n",
    "                    prev_user = curr_user\n",
    "            # Emit values of the last user in the partition\n",
    "            if curr_user and curr_user == prev_user:\n",
    "                yield prev_user, sorted(_predict_per_user(PR_u), key=lambda x: -x[1])[:top_N_ratings]\n",
    "\n",
    "        def _predict_per_partition_inner(ratings):\n",
    "            \"\"\"\n",
    "            Predict per partition if grouped option is disabled\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            DataFrame\n",
    "            \"\"\"\n",
    "            prev_user = None\n",
    "            curr_user = None\n",
    "            for r in ratings:\n",
    "                curr_user = r[\"user\"]\n",
    "                if prev_user == curr_user:\n",
    "                    PR_u[r[\"item\"]] = r[\"rating\"]\n",
    "                else:\n",
    "                    if prev_user:\n",
    "                        for el in sorted(_predict_per_user(PR_u), key=lambda x: -x[1])[:top_N_ratings]:\n",
    "                            yield Row(user=prev_user, item=el[0], rating_pred=el[1])\n",
    "                    PR_u = dict()\n",
    "                    PR_u[r[\"item\"]] = r[\"rating\"]\n",
    "                    prev_user = curr_user\n",
    "            # Emit values of the last user in the partition\n",
    "            if curr_user and curr_user == prev_user:\n",
    "                for el in sorted(_predict_per_user(PR_u), key=lambda x: -x[1])[:top_N_ratings]:\n",
    "                    yield Row(user=prev_user, item=el[0], rating_pred=el[1])\n",
    "        \n",
    "        if grouped:\n",
    "            return _predict_per_partition_inner_grouped\n",
    "        \n",
    "        return _predict_per_partition_inner\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"Save similarities or ratings in external storage\"\"\"\n",
    "        raise Exception(\"Not implemented.\")\n",
    "        \n",
    "    def evaluate(self, df_test):\n",
    "        \"\"\"RMSE for test data (optimized for large datasets)\"\"\"\n",
    "        br_similarity = getattr(self, \"br_similarity\", None)\n",
    "        df_train_local = self.df_train\n",
    "\n",
    "        # user -> list(item, rating)\n",
    "        df_user_rats = (\n",
    "            df_train_local.groupBy(\"user\")\n",
    "            .agg(F.collect_list(F.struct(\"item\", \"rating\")).alias(\"ur_list\"))\n",
    "        )\n",
    "\n",
    "        df_join = df_test.join(df_user_rats, on=\"user\", how=\"inner\")\n",
    "\n",
    "        if br_similarity is not None and len(br_similarity.value) < 5_000_000:\n",
    "            # ========== broadcast быстрый режим ==========\n",
    "            def _predict_partition(rows):\n",
    "                sim_dict = br_similarity.value\n",
    "                for r in rows:\n",
    "                    ur = r[\"ur_list\"]\n",
    "                    target_item = r[\"item\"]\n",
    "                    ws_sum = ws_den = 0.0\n",
    "                    for x in ur:\n",
    "                        j_item, r_val = x[\"item\"], x[\"rating\"]\n",
    "                        if target_item == j_item:\n",
    "                            continue\n",
    "                        k1, k2 = (target_item, j_item), (j_item, target_item)\n",
    "                        sim = sim_dict.get(k1, sim_dict.get(k2, 0.0))\n",
    "                        if sim > 0:\n",
    "                            ws_sum += sim * r_val\n",
    "                            ws_den += sim\n",
    "                    if ws_den > 0:\n",
    "                        diff2 = (r[\"rating\"] - ws_sum / ws_den) ** 2\n",
    "                        yield (1, diff2)\n",
    "\n",
    "            rdd = df_join.repartition(200).rdd.mapPartitions(_predict_partition)\n",
    "            sum_count = rdd.reduce(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
    "            return (sum_count[1] / sum_count[0]) ** 0.5 if sum_count[0] else None\n",
    "\n",
    "        # ========== distributed режим ==========\n",
    "        if getattr(self, \"df_similarity\", None) is None:\n",
    "            self.df_similarity = self.spark.read.parquet(\"hdfs:///models/similarity_matrix\").cache()\n",
    "\n",
    "        df_sim = self.df_similarity  # [p1, p2, sim]\n",
    "\n",
    "        # Чёткие алиасы, чтобы не было двусмысленности\n",
    "        df_test_expanded = (\n",
    "            df_test\n",
    "            .select(\n",
    "                F.col(\"user\"),\n",
    "                F.col(\"item\").alias(\"p1_t\"),\n",
    "                F.col(\"rating\").alias(\"true_rating\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "        df_user_items = (\n",
    "            df_train_local\n",
    "            .select(\n",
    "                F.col(\"user\"),\n",
    "                F.col(\"item\").alias(\"p2_u\"),\n",
    "                F.col(\"rating\").alias(\"rating_u\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # similarity также переименуем\n",
    "        df_sim_named = df_sim.select(\n",
    "            F.col(\"p1\").alias(\"p1_s\"),\n",
    "            F.col(\"p2\").alias(\"p2_s\"),\n",
    "            F.col(\"sim\")\n",
    "        )\n",
    "\n",
    "        # выполняем join по явным алиасам\n",
    "        df_pred = (\n",
    "            df_test_expanded.alias(\"t\")\n",
    "            .join(df_user_items.alias(\"u\"), \"user\")\n",
    "            .join(\n",
    "                df_sim_named.alias(\"s\"),\n",
    "                (F.col(\"t.p1_t\") == F.col(\"s.p1_s\")) &\n",
    "                (F.col(\"u.p2_u\") == F.col(\"s.p2_s\")),\n",
    "                \"left\"\n",
    "            )\n",
    "            .select(\n",
    "                F.col(\"t.user\"),\n",
    "                F.col(\"t.p1_t\").alias(\"item\"),\n",
    "                F.col(\"t.true_rating\"),\n",
    "                (F.col(\"s.sim\") * F.col(\"u.rating_u\")).alias(\"weighted_r\"),\n",
    "                F.col(\"s.sim\")\n",
    "            )\n",
    "            .groupBy(\"user\", \"item\", \"true_rating\")\n",
    "            .agg(\n",
    "                F.sum(\"weighted_r\").alias(\"sum_wr\"),\n",
    "                F.sum(\"sim\").alias(\"sum_sim\")\n",
    "            )\n",
    "            .withColumn(\"pred\", F.when(F.col(\"sum_sim\") > 0, F.col(\"sum_wr\") / F.col(\"sum_sim\")))\n",
    "            .filter(F.col(\"pred\").isNotNull())\n",
    "            .withColumn(\"diff2\", (F.col(\"true_rating\") - F.col(\"pred\")) ** 2)\n",
    "        )\n",
    "\n",
    "        stat = df_pred.agg(F.count(\"*\").alias(\"n\"), F.sum(\"diff2\").alias(\"s\")).collect()[0]\n",
    "        return (stat[\"s\"] / stat[\"n\"]) ** 0.5 if stat[\"n\"] else None\n",
    "    \n",
    "    def evaluate_1(self, df_test):\n",
    "        \"\"\"RMSE for test data\"\"\"\n",
    "        \n",
    "        # Разделяем данные на партиции, чтобы каждая считала свои результаты\n",
    "        br_similarity = self.br_similarity\n",
    "        df_train_local = self.df_train\n",
    "        top_items = self.br_P.value\n",
    "\n",
    "        # Подготовим удобный dataframe user-item-rating для справки\n",
    "        df_user_rats = df_train_local.groupBy(\"user\").agg(\n",
    "            F.collect_list(F.struct(\"item\", \"rating\")).alias(\"ur_list\")\n",
    "        )\n",
    "        df_join = df_test.join(df_user_rats, on=\"user\", how=\"inner\")\n",
    "\n",
    "        # Создадим функцию для оценки на каждой партиции\n",
    "        def _predict_partition(rows):\n",
    "            for r in rows:\n",
    "                ur = r[\"ur_list\"]\n",
    "                target_item = r[\"item\"]\n",
    "                ws_sum = 0.0\n",
    "                ws_den = 0.0\n",
    "                for x in ur:\n",
    "                    j_item, r_val = x[\"item\"], x[\"rating\"]\n",
    "                    if target_item == j_item:\n",
    "                        continue\n",
    "                    k1, k2 = (target_item, j_item), (j_item, target_item)\n",
    "                    sim = 0.0\n",
    "                    if k1 in br_similarity.value:\n",
    "                        sim = br_similarity.value[k1]\n",
    "                    elif k2 in br_similarity.value:\n",
    "                        sim = br_similarity.value[k2]\n",
    "                    if sim > 0:\n",
    "                        ws_sum += sim * r_val\n",
    "                        ws_den += sim\n",
    "                pred = ws_sum / ws_den if ws_den > 0 else None\n",
    "                if pred is not None:\n",
    "                    diff2 = (r[\"rating\"] - pred) ** 2\n",
    "                    yield (1, diff2)  # счетчик и сумма квадратов ошибок\n",
    "\n",
    "        # Применяем mapPartitions → редукция\n",
    "        rdd = df_join.repartition(200).rdd.mapPartitions(_predict_partition)\n",
    "        sum_count = rdd.reduce(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
    "        if sum_count[0] == 0:\n",
    "            return None\n",
    "        rmse = (sum_count[1] / sum_count[0]) ** 0.5\n",
    "        return rmse\n",
    "\n",
    "    def recommend_and_show(self, num_rows_to_display=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Launch the action operation to calculate recommendations, collect data\n",
    "        on the driver and display them.\n",
    "        \"\"\"\n",
    "        if num_rows_to_display:\n",
    "            num_rows_to_display = int(num_rows_to_display)\n",
    "\n",
    "        result = self.recommend(**kwargs)\n",
    "\n",
    "        if isinstance(result, RDD):\n",
    "            return result.take(num_rows_to_display) if num_rows_to_display else result.collect()\n",
    "        elif isinstance(result, DataFrame):\n",
    "            return result.show(num_rows_to_display) if num_rows_to_display else result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"itemBasedRecommendationTest\") \\\n",
    "        .config(\"spark.driver.memory\", \"32g\") \\\n",
    "        .config(\"spark.executor.memory\", \"24g\") \\\n",
    "        .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "        .config(\"spark.memory.storageFraction\", \"0.3\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"2000\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mdataset\u001b[00m\n",
      "├── \u001b[01;34mml-latest\u001b[00m\n",
      "│   ├── genome-scores.csv\n",
      "│   ├── genome-tags.csv\n",
      "│   ├── links.csv\n",
      "│   ├── movies.csv\n",
      "│   ├── ratings.csv\n",
      "│   ├── README.txt\n",
      "│   └── tags.csv\n",
      "└── \u001b[01;34mml-latest-small\u001b[00m\n",
      "    ├── links.csv\n",
      "    ├── movies.csv\n",
      "    ├── ratings.csv\n",
      "    ├── README.txt\n",
      "    └── tags.csv\n",
      "\n",
      "2 directories, 12 files\n",
      "-rw-r--r-- 1 ubuntu ubuntu 2.4M Sep 26  2018 dataset/ml-latest-small/ratings.csv\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 891M Jul 20  2023 dataset/ml-latest/ratings.csv\n"
     ]
    }
   ],
   "source": [
    "#!sudo apt install tree\n",
    "!tree dataset\n",
    "!ls -lah dataset/ml-latest-small/ratings.csv\n",
    "!ls -lah dataset/ml-latest/ratings.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"/home/ubuntu/Documents/BigDataHWs/hw4/dataset\"\n",
    "\n",
    "SMALL_RATINGS_FILE = f\"file://{BASE_PATH}/ml-latest-small/ratings.csv\"\n",
    "FULL_RATINGS_FILE = f\"file://{BASE_PATH}/ml-latest/ratings.csv\"\n",
    "\n",
    "RATINGS_FILE = SMALL_RATINGS_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rating = spark.read.load(RATINGS_FILE,\n",
    "                          format=\"csv\",\n",
    "                          header=\"true\",\n",
    "                          inferSchema=\"true\",\n",
    "                          sep=\",\")\n",
    "\n",
    "df_rating.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of ratings: 100836\n",
      "+----+----+------+\n",
      "|user|item|rating|\n",
      "+----+----+------+\n",
      "|   1|   1|   4.0|\n",
      "|   1|   3|   4.0|\n",
      "|   1|   6|   4.0|\n",
      "|   1|  47|   5.0|\n",
      "|   1|  50|   5.0|\n",
      "+----+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rating_true = df_rating.\\\n",
    "    drop(\"timestamp\").\\\n",
    "    withColumnRenamed(\"userId\", \"user\").\\\n",
    "    withColumnRenamed(\"movieId\", \"item\")\n",
    "\n",
    "print(\"Total number of ratings:\", df_rating_true.count())\n",
    "df_rating_true.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user: int, item: int, rating: double]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train, df_test = df_rating_true.randomSplit([0.8, 0.2], seed=7)\n",
    "df_train.persist(); df_test.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+\n",
      "|user|item|rating|\n",
      "+----+----+------+\n",
      "|   1|   3|   4.0|\n",
      "|   1|   6|   4.0|\n",
      "|   1|  47|   5.0|\n",
      "|   1|  50|   5.0|\n",
      "|   1|  70|   3.0|\n",
      "+----+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средний рейтинг фильмов в обучающем множестве: 3.5033925166530473\n"
     ]
    }
   ],
   "source": [
    "mean_rating = df_train.agg(F.avg('rating')).collect()[0][0]\n",
    "\n",
    "print(f'Средний рейтинг фильмов в обучающем множестве: {mean_rating}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+------------------+------------------+\n",
      "|user|item|rating|              pred|      SquaredError|\n",
      "+----+----+------+------------------+------------------+\n",
      "|   1|   1|   4.0|3.5033925166530473|0.2466189925161939|\n",
      "|   1| 157|   5.0|3.5033925166530473|2.2398339592100993|\n",
      "|   1| 235|   4.0|3.5033925166530473|0.2466189925161939|\n",
      "|   1| 260|   5.0|3.5033925166530473|2.2398339592100993|\n",
      "|   1| 362|   5.0|3.5033925166530473|2.2398339592100993|\n",
      "+----+----+------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test_predict_mean = df_test.\\\n",
    "                        withColumn('pred', F.lit(mean_rating)).\\\n",
    "                        withColumn('SquaredError', (F.col(\"rating\") - F.col(\"pred\")) ** 2)\n",
    "\n",
    "df_test_predict_mean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE, когда все предсказания равны среднему рейтингу: 1.0500201634697948\n"
     ]
    }
   ],
   "source": [
    "rmse_base = (df_test_predict_mean.agg(F.sum('SquaredError')).collect()[0][0] / df_test_predict_mean.count()) ** 0.5\n",
    "\n",
    "print(f\"RMSE, когда все предсказания равны среднему рейтингу: {rmse_base}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column<b'user AS `user`'>,\n",
       " Column<b'item AS `item`'>,\n",
       " Column<b'rating AS `rating`'>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_N = 20\n",
    "\n",
    "user_column_name=\"user\"\n",
    "item_column_name=\"item\"\n",
    "rating_column_name=\"rating\"\n",
    "\n",
    "clmn_names = [F.col(user_column_name).alias(\"user\"),\n",
    "                      F.col(item_column_name).alias(\"item\"),\n",
    "                      F.col(rating_column_name).alias(\"rating\")]\n",
    "\n",
    "clmn_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+\n",
      "|user|item|rating|\n",
      "+----+----+------+\n",
      "|   1|   3|   4.0|\n",
      "|   1|   6|   4.0|\n",
      "|   1|  47|   5.0|\n",
      "|   1|  50|   5.0|\n",
      "|   1|  70|   3.0|\n",
      "+----+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train.select(clmn_names)\n",
    "df_train.persist()\n",
    "df_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_clmn_names = [F.col(\"user\").alias(\"u\"),\n",
    "           F.col(\"item\").alias(\"p1\"),\n",
    "           F.col(\"rating\").alias(\"v1\")]\n",
    "\n",
    "right_clmn_names = [F.col(\"user\").alias(\"u\"),\n",
    "            F.col(\"item\").alias(\"p2\"),\n",
    "            F.col(\"rating\").alias(\"v2\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+\n",
      "|  p1|  p2|   dot|\n",
      "+----+----+------+\n",
      "| 101|2033|  35.5|\n",
      "| 590|2174|349.25|\n",
      "|1031|3439|  32.0|\n",
      "|1282|2450|  56.5|\n",
      "|1617|2916| 349.0|\n",
      "+----+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1. Create dot products\n",
    "\n",
    "df_dot = df_train.select(left_clmn_names) \\\n",
    "    .join(df_train.select(right_clmn_names), on=\"u\") \\\n",
    "    .where(F.col(\"p1\") < F.col(\"p2\")) \\\n",
    "    .groupBy([F.col(\"p1\"), F.col(\"p2\")]) \\\n",
    "    .agg(F.sum(F.col(\"v1\") * F.col(\"v2\")).alias(\"dot\"))\n",
    "\n",
    "df_dot.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|  p1|              norm|\n",
      "+----+------------------+\n",
      "|2366|15.827191791344413|\n",
      "|1088|21.418449990603897|\n",
      "|4519| 9.772410142846033|\n",
      "|1591|12.338962679253067|\n",
      "|3918|10.307764064044152|\n",
      "+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2. Calculate norms\n",
    "\n",
    "df_norm = df_train \\\n",
    "    .withColumn(\"sq\", F.col(\"rating\") ** 2) \\\n",
    "    .groupBy(\"item\") \\\n",
    "    .agg(F.sqrt(F.sum(\"sq\")).alias(\"norm\")) \\\n",
    "    .withColumnRenamed(\"item\", \"p1\")\n",
    "\n",
    "# df_norm = df_train.select(left_clmn_names) \\\n",
    "#     .groupBy(F.col(\"p1\")) \\\n",
    "#     .agg(F.sqrt(F.sum(F.col(\"v1\") * F.col(\"v1\"))).alias(\"norm\"))\n",
    "\n",
    "df_norm.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column<b'p1'>, Column<b'p2'>, Column<b'((dot / n1) / n2) AS `sim`'>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_clmns = [F.col(\"p1\"), F.col(\"p2\"), (F.col(\"dot\")/F.col(\"n1\")/F.col(\"n2\")).alias(\"sim\")]\n",
    "\n",
    "similarity_clmns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.72 µs\n",
      "+----+----+-------------------+\n",
      "|  p1|  p2|                sim|\n",
      "+----+----+-------------------+\n",
      "| 101|2033|0.23471106442350215|\n",
      "| 590|2174|0.24817296741926048|\n",
      "|1031|3439|0.20931133554085943|\n",
      "|1282|2450|0.28442234751656525|\n",
      "|1617|2916| 0.3069735548256318|\n",
      "+----+----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# Step 3. Calculate similarities\n",
    "\n",
    "# # Проверим фактические имена столбцов в df_norm\n",
    "# print(df_norm.columns)\n",
    "# # Допустим, вывод: ['p1', 'norm'] (если столбец уже p1),\n",
    "# # либо ['item', 'norm'] — подставляем автоматически.\n",
    "\n",
    "# # Унифицируем во временном df_norm_fixed с нужными именами:\n",
    "# if \"item\" in df_norm.columns:\n",
    "#     df_norm_fixed = df_norm.select(\n",
    "#         F.col(\"item\").alias(\"item_key\"),\n",
    "#         F.col(\"norm\")\n",
    "#     )\n",
    "# else:\n",
    "#     df_norm_fixed = df_norm.select(\n",
    "#         F.col(\"p1\").alias(\"item_key\"),\n",
    "#         F.col(\"norm\")\n",
    "#     )\n",
    "\n",
    "# # Один широковещательный экземпляр, чтобы не гнать shuffle\n",
    "# df_norm_b = F.broadcast(df_norm_fixed)\n",
    "\n",
    "# # Явные объекты для join с уникальными именами\n",
    "# norm_p1 = df_norm_b.select(\n",
    "#     F.col(\"p1\").alias(\"p1_key\"),\n",
    "#     F.col(\"norm\").alias(\"n1\")\n",
    "# )\n",
    "# norm_p2 = df_norm_b.select(\n",
    "#     F.col(\"p2\").alias(\"p2_key\"),\n",
    "#     F.col(\"norm\").alias(\"n2\")\n",
    "# )\n",
    "\n",
    "# # Основной расчёт\n",
    "# df_similarity = (\n",
    "#     df_dot\n",
    "#     .join(norm_p1, df_dot.p1 == norm_p1.p1_key, \"inner\")\n",
    "#     .join(norm_p2, df_dot.p2 == norm_p2.p2_key, \"inner\")\n",
    "#     .select(\n",
    "#         df_dot.p1.alias(\"p1\"),\n",
    "#         df_dot.p2.alias(\"p2\"),\n",
    "#         (F.col(\"dot\") / (F.col(\"n1\") * F.col(\"n2\"))).alias(\"sim\")\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# df_similarity.persist()\n",
    "\n",
    "df_norm_cached = df_norm.persist()\n",
    "\n",
    "df_similarity = df_dot \\\n",
    "    .join(df_norm_cached.select(F.col(\"p1\"), F.col(\"norm\").alias(\"n1\")), on=\"p1\") \\\n",
    "    .join(df_norm_cached.select(F.col(\"p1\").alias(\"p2\"), F.col(\"norm\").alias(\"n2\")), on=\"p2\") \\\n",
    "    .select(similarity_clmns)\n",
    "\n",
    "df_similarity.persist()\n",
    "\n",
    "df_similarity.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9765973"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_similarity.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4. Truncate similarities\n",
    "\n",
    "# window = Window.partitionBy(df_similarity[\"p1\"]).orderBy(df_similarity[\"sim\"].desc())\n",
    "# df_similarity_N = df_similarity.select(\"*\", F.rank().over(window).alias(\"rank\"))\\\n",
    "#             .filter(F.col(\"rank\") <= top_N)\n",
    "\n",
    "# df_similarity_N.where(F.col(\"p1\") == 5943).show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction of rating for user 123 for movie 96829:\n",
      "4.1205706441353716\n",
      "+----+------+------+-----+------+-------------------+\n",
      "|user|  item|rating|   p1|    p2|                sim|\n",
      "+----+------+------+-----+------+-------------------+\n",
      "| 123|104879|   4.0|96829|104879| 0.3209761539224515|\n",
      "| 123|112552|   3.5|96829|112552| 0.2997193396242945|\n",
      "| 123| 68554|   4.5|68554| 96829|0.25986315314949343|\n",
      "| 123| 91542|   4.0|91542| 96829| 0.2505657519209857|\n",
      "| 123|109487|   4.5|96829|109487|0.24915196995329322|\n",
      "| 123| 51540|   4.0|51540| 96829|0.22468608298819884|\n",
      "| 123| 99114|   4.5|96829| 99114|0.22222097541033137|\n",
      "| 123|  2959|   4.5| 2959| 96829|0.21772448600571395|\n",
      "| 123| 58803|   4.5|58803| 96829|0.20076712259557772|\n",
      "| 123|  6016|   3.5| 6016| 96829| 0.1981394871073563|\n",
      "| 123|112556|   4.5|96829|112556|0.19346644054835044|\n",
      "| 123| 60684|   4.0|60684| 96829|  0.186581493359926|\n",
      "| 123| 68157|   4.0|68157| 96829|0.17774415423474987|\n",
      "| 123| 91500|   3.5|91500| 96829|0.17316948846420208|\n",
      "| 123|115569|   4.5|96829|115569|0.16262043771744306|\n",
      "| 123|   593|   4.0|  593| 96829|0.16035253514854117|\n",
      "| 123| 64969|   4.0|64969| 96829|0.15285979159776877|\n",
      "| 123|116797|   4.5|96829|116797| 0.1507026708633252|\n",
      "| 123| 76093|   4.5|76093| 96829|0.14704206468955905|\n",
      "| 123|104913|   3.5|96829|104913|0.14536615126598554|\n",
      "+----+------+------+-----+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "\n",
    "user = 123\n",
    "movieId = 96829\n",
    "\n",
    "df_sim_filtered = df_similarity\\\n",
    "    .filter((F.col(\"p1\") == movieId) | (F.col('p2') == movieId)).sort('p2', ascending=True)\n",
    "\n",
    "df_predict_example = df_train\\\n",
    "    .where(F.col('user') == user)\\\n",
    "    .join(df_similarity, ((df_train.item == df_sim_filtered.p1) & (df_sim_filtered.p2 == movieId)) | \n",
    "                         ((df_train.item == df_sim_filtered.p2) & (df_sim_filtered.p1 == movieId))) \\\n",
    "    .sort('sim', ascending=False) \\\n",
    "    .limit(top_N)\n",
    "\n",
    "df_predict_example2 = df_train.where(F.col('user') == user).join(df_norm, df_norm.p1 == df_train.item)\n",
    "\n",
    "w_sum = df_predict_example\\\n",
    "    .select(F.sum(F.col(\"rating\") * F.col(\"sim\")).alias(\"weighted_sum\"))\\\n",
    "    .collect()[0][\"weighted_sum\"]\n",
    "\n",
    "w_norms = df_predict_example.agg(F.sum('sim')).collect()[0][0]\n",
    "\n",
    "print(f\"Prediction of rating for user {user} for movie {movieId}:\")\n",
    "print(w_sum / w_norms)\n",
    "\n",
    "df_predict_example.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n",
      "|user| item|rating|\n",
      "+----+-----+------+\n",
      "| 123|  293|   3.5|\n",
      "| 123| 2329|   4.5|\n",
      "| 123|96829|   4.5|\n",
      "| 123|97923|   4.0|\n",
      "+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.where(F.col('user') == 123).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemBasedCF:\n",
    "    \n",
    "    def __init__(self, spark, top_N_similarities=20, top_N_ratings=None):\n",
    "        self.spark = spark\n",
    "        self.top_N_similarities = int(top_N_similarities)\n",
    "        self.top_N_ratings = int(top_N_ratings) if top_N_ratings else None\n",
    "        \n",
    "    def train(self, df_train, top_N=None, user_column_name=\"user\", item_column_name=\"item\",\n",
    "              rating_column_name=\"rating\"):\n",
    "        top_N = int(top_N) if top_N else self.top_N_similarities\n",
    "        user_column_name = str(user_column_name)\n",
    "        item_column_name = str(item_column_name)\n",
    "        rating_column_name = str(rating_column_name)\n",
    "        \n",
    "        clmn_names = [F.col(user_column_name).alias(\"user\"),\n",
    "                      F.col(item_column_name).alias(\"item\"),\n",
    "                      F.col(rating_column_name).alias(\"rating\")]\n",
    "        \n",
    "        df_train = df_train.select(clmn_names)\n",
    "        df_train.persist()\n",
    "        \n",
    "        left_clmn_names = [F.col(\"user\").alias(\"u\"),\n",
    "           F.col(\"item\").alias(\"p1\"),\n",
    "           F.col(\"rating\").alias(\"v1\")]\n",
    "\n",
    "        right_clmn_names = [F.col(\"user\").alias(\"u\"),\n",
    "                    F.col(\"item\").alias(\"p2\"),\n",
    "                    F.col(\"rating\").alias(\"v2\")]\n",
    "        \n",
    "        # Step 1. Create dot products\n",
    "        df_dot = df_train.select(left_clmn_names) \\\n",
    "            .join(df_train.select(right_clmn_names), on=\"u\") \\\n",
    "            .where(F.col(\"p1\") < F.col(\"p2\")) \\\n",
    "            .groupBy([F.col(\"p1\"), F.col(\"p2\")]) \\\n",
    "            .agg(F.sum(F.col(\"v1\") * F.col(\"v2\")).alias(\"dot\"))\n",
    "        \n",
    "        # Step 2. Calculate norms\n",
    "        df_norm = df_train \\\n",
    "            .withColumn(\"sq\", F.col(\"rating\") ** 2) \\\n",
    "            .groupBy(\"item\") \\\n",
    "            .agg(F.sqrt(F.sum(\"sq\")).alias(\"norm\")) \\\n",
    "            .withColumnRenamed(\"item\", \"p1\")\n",
    "        \n",
    "        similarity_clmns = [F.col(\"p1\"), F.col(\"p2\"), (F.col(\"dot\")/F.col(\"n1\")/F.col(\"n2\")).alias(\"sim\")]\n",
    "        \n",
    "        # Step 3. Calculate similarities\n",
    "        df_norm_cached = F.broadcast(df_norm)\n",
    "\n",
    "        df_similarity = df_dot \\\n",
    "            .join(df_norm_cached.select(F.col(\"p1\"), F.col(\"norm\").alias(\"n1\")), on=\"p1\") \\\n",
    "            .join(df_norm_cached.select(F.col(\"p1\").alias(\"p2\"), F.col(\"norm\").alias(\"n2\")), on=\"p2\") \\\n",
    "            .select(similarity_clmns)\n",
    "\n",
    "        df_similarity.persist()\n",
    "        \n",
    "        self.df_train = df_train\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, user_id, item_id):\n",
    "        \"\"\"Predict rating for a given user and item\"\"\"\n",
    "        user = user_id\n",
    "        movieId = item_id\n",
    "\n",
    "        df_sim_filtered = df_similarity\\\n",
    "            .filter((F.col(\"p1\") == movieId) | (F.col('p2') == movieId)).sort('p2', ascending=True)\n",
    "\n",
    "        df_predict_example = df_train\\\n",
    "            .where(F.col('user') == user)\\\n",
    "            .join(df_similarity, ((df_train.item == df_sim_filtered.p1) & (df_sim_filtered.p2 == movieId)) | \n",
    "                                 ((df_train.item == df_sim_filtered.p2) & (df_sim_filtered.p1 == movieId))) \\\n",
    "            .sort('sim', ascending=False) \\\n",
    "            .limit(top_N)\n",
    "\n",
    "        df_predict_example2 = df_train.where(F.col('user') == user).join(df_norm, df_norm.p1 == df_train.item)\n",
    "\n",
    "        w_sum = df_predict_example\\\n",
    "            .select(F.sum(F.col(\"rating\") * F.col(\"sim\")).alias(\"weighted_sum\"))\\\n",
    "            .collect()[0][\"weighted_sum\"]\n",
    "\n",
    "        w_norms = df_predict_example.agg(F.sum('sim')).collect()[0][0]\n",
    "        \n",
    "        return w_sum / w_norms\n",
    "    \n",
    "     def evaluate(self, df_test):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ItemBasedRecommend(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_cf = model.evaluate(df_test)\n",
    "print(f\"RMSE модели item-based CF: {rmse_cf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ItemBasedRecommend(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.recommend_and_show(user_ids=model.br_U.value, grouped=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
